{
  
    
        "post0": {
            "title": "Lunar Lander with Deep Q-Network",
            "content": "Introduction . Way back in 2013, DeepMind presented the Deep Q-Network (DQN) agent applied to Atari 2600 games. This agent grabbed screenshots from Atari games as input and used Q-learning to predict and take the best action. With traditional Q-learning, the entire state space must be represented in the Q-table, the table that stores state-action pairs with their Q-values. DQN uses a neural network as a function estimator to estimate this Q-fuction, rather than storing the Q-values explicitely. (Mnih et al., 2013) . Here, we&#39;ll implement a simplified version of the DQN agent applied to the Gym Lunar Lander environment. . import gym import torch import torch.nn as nn import torch.nn.functional as F import pandas as pd import random import numpy as np import matplotlib import matplotlib.pyplot as plt from itertools import count from collections import deque, namedtuple COLAB = &#39;google.colab&#39; in str(get_ipython()) if COLAB: !pip install box2d is_ipython = &#39;inline&#39; in matplotlib.get_backend() if is_ipython: from IPython import display . SEED = 42 . class DQN(nn.Module): &quot;&quot;&quot; Class adapted from PyTorch example: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html &quot;&quot;&quot; def __init__(self, inputs, outputs): super().__init__() self.fc1 = nn.Linear(in_features=inputs, out_features=64) self.fc2 = nn.Linear(in_features=64, out_features=64) self.fc3 = nn.Linear(in_features=64, out_features=64) self.fc4 = nn.Linear(in_features=64, out_features=32) self.out = nn.Linear(in_features=32, out_features=outputs) def forward(self, t): t = F.relu(self.fc1(t)) t = F.relu(self.fc2(t)) t = F.relu(self.fc3(t)) t = F.relu(self.fc4(t)) t = self.out(t) return t . Experience = namedtuple(&quot;Experience&quot;, field_names=[ &quot;state&quot;, &quot;action&quot;, &quot;reward&quot;, &quot;next_state&quot;, &quot;done&quot;]) class ReplayMemory(object): &quot;&quot;&quot; Class adapted from PyTorch example: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html &quot;&quot;&quot; def __init__(self, buffer_size, batch_size, seed): self.memory = deque(maxlen=buffer_size) self.batch_size = batch_size self.seed = random.seed(seed) def push(self, state, action, reward, next_state, done): self.memory.append(Experience(state, action, reward, next_state, done)) def sample(self, device): &quot;&quot;&quot; Sample a set memories. Code adapted from a post from Chanseok Kang: https://goodboychan.github.io/python/reinforcement_learning/pytorch/udacity/2021/05/07/DQN-LunarLander.html &quot;&quot;&quot; experiences = random.sample(self.memory, k=self.batch_size) states = torch.from_numpy( np.vstack([e.state for e in experiences if e is not None])).float().to(device) actions = torch.from_numpy( np.vstack([e.action for e in experiences if e is not None])).long().to(device) rewards = torch.from_numpy( np.vstack([e.reward for e in experiences if e is not None])).float().to(device) next_states = torch.from_numpy(np.vstack( [e.next_state for e in experiences if e is not None])).float().to(device) dones = torch.from_numpy(np.vstack( [e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device) return (states, actions, rewards, next_states, dones) def __len__(self): return len(self.memory) . class DQNAgent(): &quot;&quot;&quot; Class adapted from PyTorch example: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html &quot;&quot;&quot; def __init__( self, state_vector_length, num_actions, alpha=.001, eps=1, eps_decay=0.995, eps_min=0.05, gamma=0.9, batch_size=64, seed=None ): self.num_actions = num_actions self.eps = eps self.eps_decay = eps_decay self.eps_min = eps_min self.gamma = gamma self.device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) self.step = 0 self.policy_net = DQN(state_vector_length, num_actions).to(self.device) self.target_net = DQN(state_vector_length, num_actions).to(self.device) self.target_net.load_state_dict(self.policy_net.state_dict()) self.target_net.eval() self.optimizer = torch.optim.Adam( params=self.policy_net.parameters(), lr=alpha) self.memory = ReplayMemory(100000, batch_size, seed) if seed != None: np.random.seed(seed) def select_action(self, s): self.step += 1 if np.random.random() &lt; self.eps: action = np.random.randint(0, self.num_actions) else: action = self._get_best_action(s) return action def _get_best_action(self, s): with torch.no_grad(): action = self.policy_net(torch.tensor([s]).to( self.device)).argmax(dim=1).to(self.device).item() return action def update_q(self, s, a, s_prime, r, done): self.memory.push(s, a, r, s_prime, done) self.step += 1 if done: self.eps = max(self.eps_min, self.eps * self.eps_decay) if len(self.memory) &gt; self.memory.batch_size: experiences = self.memory.sample(self.device) self.learn(experiences) def learn(self, experiences): states, actions, rewards, next_states, dones = experiences next_q_values = self.target_net( next_states).detach().max(1)[0].unsqueeze(1) q_targets = rewards + self.gamma * next_q_values * (1 - dones) current_q_values = self.policy_net(states).gather(1, actions) loss = F.mse_loss(current_q_values, q_targets) self.optimizer.zero_grad() loss.backward() self.optimizer.step() def update_target(self): self.target_net.load_state_dict(self.policy_net.state_dict()) def save_network(self, outfile): torch.save(self.policy_net.state_dict(), outfile) def load_network(self, infile): self.policy_net.load_state_dict(torch.load(infile)) self.policy_net.eval() . class SimpleDQNAgent(): def __init__( self, state_vector_length, num_actions, alpha=.001, eps=1, eps_decay=0.995, eps_min=0.05, gamma=0.9, batch_size=64, seed=None ): self.num_actions = num_actions self.eps = eps self.eps_decay = eps_decay self.eps_min = eps_min self.gamma = gamma self.device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) self.step = 0 self.policy_net = DQN(state_vector_length, num_actions).to(self.device) self.optimizer = torch.optim.Adam( params=self.policy_net.parameters(), lr=alpha) self.memory = ReplayMemory(100000, batch_size, seed) if seed != None: np.random.seed(seed) def select_action(self, s): self.step += 1 if np.random.random() &lt; self.eps: action = np.random.randint(0, self.num_actions) else: action = self._get_best_action(s) return action def _get_best_action(self, s): with torch.no_grad(): action = self.policy_net(torch.tensor([s]).to( self.device)).argmax(dim=1).to(self.device).item() return action def update_q(self, s, a, s_prime, r, done): self.memory.push(s, a, r, s_prime, done) self.step += 1 if done: self.eps = max(self.eps_min, self.eps * self.eps_decay) if len(self.memory) &gt; self.memory.batch_size: experiences = self.memory.sample(self.device) self.learn(experiences) def learn(self, experiences): states, actions, rewards, next_states, dones = experiences next_q_values = self.policy_net( next_states).detach().max(1)[0].unsqueeze(1) q_targets = rewards + self.gamma * next_q_values * (1 - dones) current_q_values = self.policy_net(states).gather(1, actions) loss = F.mse_loss(current_q_values, q_targets) self.optimizer.zero_grad() loss.backward() self.optimizer.step() def save_network(self, outfile): torch.save(self.policy_net.state_dict(), outfile) def load_network(self, infile): self.policy_net.load_state_dict(torch.load(infile)) self.policy_net.eval() . def moving_average(data, window): series = pd.Series(data) return series.rolling(window).mean() . def plot_rewards(values): plt.figure(2) plt.clf() plt.xlabel(&#39;Episode&#39;) plt.ylabel(&#39;Reward&#39;) plt.plot(values) plt.plot(moving_average(values, 100)) . def plot_multiple_rewards(variable, rewards_dict): plt.figure(2) plt.clf() plt.xlabel(&#39;Episode&#39;) plt.ylabel(&#39;Reward&#39;) for key, rewards in rewards_dict.items(): plt.plot(rewards, label=f&#39;{variable} = {key}&#39;) plt.legend() . def lander_runner(num_episodes, target_update, alpha, eps, eps_decay, gamma, seed, convergence_threshold=200, render=False): env = gym.make(&#39;LunarLander-v2&#39;) env.seed(SEED) agent = DQNAgent(env.observation_space.shape[0], env.action_space.n, alpha=alpha, eps=eps, eps_decay=eps_decay, gamma=gamma, seed=SEED) rewards = [] for e in range(num_episodes): cur_observation = env.reset() if render: env.render() episode_reward = 0 for t in count(): action = agent.select_action(cur_observation) next_observation, reward, done, info = env.step(action) agent.update_q(cur_observation, action, next_observation, reward, done) cur_observation = next_observation episode_reward += reward if render: env.render() if done: rewards.append(episode_reward) plot_rewards(rewards) plt.pause(0.01) print(f&#39;Episode {e}: {episode_reward}&#39;) if is_ipython: display.clear_output(wait=True) break if e % target_update == 0: agent.update_target() if np.all(moving_average(rewards, 100)[-100:] &gt;= convergence_threshold): print(f&#39;Solved in {e} episodes.&#39;) agent.save_network(f&#39;out agent.pt&#39;) break env.close() return rewards, agent . run_rewards, agent = lander_runner( num_episodes=1500, target_update=4, alpha=0.0005, eps=1, eps_decay=0.99, gamma=0.999, seed=57, convergence_threshold=210 ) plot_rewards(run_rewards) plt.savefig(&#39;out learning_curve.png&#39;) . Solved in 824 episodes. . env = gym.make(&#39;LunarLander-v2&#39;) agent.load_network(&#39;out agent.pt&#39;) agent.policy_net.eval() observation = env.reset() render = False rewards = [] for e in range(100): cur_observation = env.reset() if render: env.render() episode_reward = 0 for t in count(): action = agent.select_action(cur_observation) next_observation, reward, done, info = env.step(action) cur_observation = next_observation episode_reward += reward if render: env.render() if done: rewards.append(episode_reward) plot_rewards(rewards) plt.pause(0.01) print(f&#39;Episode {e}: {episode_reward}&#39;) if is_ipython: display.clear_output(wait=True) break env.close() plt.figure(2) plt.clf() plt.xlabel(&#39;Episode&#39;) plt.ylabel(&#39;Reward&#39;) plt.plot(rewards) plt.axhline(y=np.mean(rewards), color=&#39;r&#39;, linestyle=&#39;-&#39;) plt.savefig(&#39;out agent_performance.png&#39;) . alphas = [0.01, 0.005, 0.001, 0.0005, 0.0001] rewards_dict = {} for a in alphas: run_rewards, agent = lander_runner( num_episodes=1500, target_update=10, alpha=a, eps=1, eps_decay=0.99, gamma=0.999, seed=42 ) rewards_dict[a] = moving_average(run_rewards, 100) plot_multiple_rewards(&#39; u03B1&#39;, rewards_dict) plt.savefig(&#39;out alpha_learning_curve.png&#39;) . Solved in 1063 episodes. . gamma = [0.9, 0.99, 0.995, 0.999] rewards_dict = {} for g in gamma: run_rewards, agent = lander_runner( num_episodes=1500, target_update=10, alpha=0.0005, eps=1, eps_decay=0.99, gamma=g, seed=42 ) rewards_dict[g] = moving_average(run_rewards, 100) plot_multiple_rewards(&#39; u03B3&#39;, rewards_dict) plt.savefig(&#39;out gamma_learning_curve.png&#39;) . Solved in 835 episodes. . target_update = [2, 4, 8, 10] rewards_dict = {} for tu in target_update: run_rewards, agent = lander_runner( num_episodes=1500, target_update=tu, alpha=0.0005, eps=1, eps_decay=0.99, gamma=0.999, seed=42 ) rewards_dict[tu] = moving_average(run_rewards, 100) plot_multiple_rewards(&#39;Target Network Update&#39;, rewards_dict) plt.savefig(&#39;out target_update_learning_curve.png&#39;) . Solved in 935 episodes. . def simple_lander_runner(num_episodes, target_update, alpha, eps, eps_decay, gamma, seed, render=False): env = gym.make(&#39;LunarLander-v2&#39;) env.seed(seed) agent = SimpleDQNAgent(env.observation_space.shape[0], env.action_space.n, alpha=alpha, eps=eps, eps_decay=eps_decay, gamma=gamma, seed=seed) rewards = [] for e in range(num_episodes): cur_observation = env.reset() if render: env.render() episode_reward = 0 for t in count(): action = agent.select_action(cur_observation) next_observation, reward, done, info = env.step(action) agent.update_q(cur_observation, action, next_observation, reward, done) cur_observation = next_observation episode_reward += reward if render: env.render() if done: rewards.append(episode_reward) plot_rewards(rewards) plt.pause(0.01) print(f&#39;Episode {e}: {episode_reward}&#39;) if is_ipython: display.clear_output(wait=True) break if np.all(moving_average(rewards, 100)[-100:] &gt;= 200): print(f&#39;Solved in {e} episodes.&#39;) agent.save_network(f&#39;out agent.pt&#39;) break env.close() return rewards, agent . run_rewards, agent = simple_lander_runner( num_episodes=1500, target_update=4, alpha=0.0005, eps=1, eps_decay=0.99, gamma=0.999, seed=57 ) plot_rewards(run_rewards) plt.savefig(&#39;out simple_learning_curve.png&#39;) . Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., &amp; Riedmiller, M. (2013). Playing Atari with Deep Reinforcement Learning. https://arxiv.org/abs/1312.5602 | .",
            "url": "https://kylejackson.org/dqn/rl/2021/12/29/lunar_lander.html",
            "relUrl": "/dqn/rl/2021/12/29/lunar_lander.html",
            "date": " • Dec 29, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://kylejackson.org/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://kylejackson.org/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Kyle. This blog is a place for me to explore various machine learning research areas. I’ll also explore some various software engineering topics of interest. .",
          "url": "https://kylejackson.org/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://kylejackson.org/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}